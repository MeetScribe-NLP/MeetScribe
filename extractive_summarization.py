# -*- coding: utf-8 -*-
"""Extractive_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y96HFPWNbOrSJGLTfFAr5fr_2wUVp3g4
"""

import gzip
import pandas as pd

gz_file_path = 'meetingBank_styled.csv.gz'
csv_output_path = 'meetingBank_styled.csv'

# Step 1: Uncompress the .gz file
with gzip.open(gz_file_path, 'rt') as f_in:
    with open(csv_output_path, 'w', encoding='utf-8') as f_out:
        f_out.writelines(f_in)

# Step 2: Read the resulting CSV file (optional)
df = pd.read_csv(csv_output_path)

df.head()

df.info()

print(df.columns)

# ---- Install Dependencies ----
import sys
import subprocess
import nltk
from tqdm.notebook import tqdm

required_libs = ['summa', 'sumy', 'nltk', 'transformers>=4.18.0', 'sentencepiece']
for lib in required_libs:
    try:
        __import__(lib.split('==')[0])
    except ModuleNotFoundError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", lib],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)

nltk.download('punkt', quiet=True)

# ---- Optimized Imports ----
import pandas as pd
from summa import summarizer as textrank_summarize
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer
import torch
from transformers import pipeline

# ---- Data Loading ----
try:
    df = pd.read_csv('meetingBank_styled.csv', nrows=500)
    # Ensure transcript column exists
    if 'transcript' not in df.columns:
        df['transcript'] = ''
except Exception:
    df = pd.DataFrame(columns=['transcript'])

# ---- Initialize Models ----
lex_summarizer = LexRankSummarizer()
bert_available = False

# ---- GPU Optimization ----
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    device = 0
else:
    device = -1

# ---- BERT Initialization ----
try:
    bertsum_pipe = pipeline(
        "summarization",
        model="sshleifer/distilbart-cnn-12-6",
        device=device,
        framework="pt",
        truncation=True
    )
    # Warm-up
    if device == 0:
        _ = bertsum_pipe("Warm up", max_length=30, min_length=10)
    bert_available = True
except Exception as e:
    print(f"BERT initialization failed: {str(e)}")

# ---- Pre-allocated Results ----
results_with_meta = [None] * len(df)
results_without_meta = [None] * len(df)

# ---- Optimized Processing Functions ----
def preprocess_text(text):
    """Faster text preprocessing"""
    if not isinstance(text, str):
        return ""
    text = text.strip()
    return text if len(text.split()) >= 3 else ""

def generate_textrank(text):
    text = preprocess_text(text)
    if not text:
        return ""
    try:
        return textrank_summarize.summarize(text, ratio=0.3) or text[:300] + "..."
    except Exception:
        return text[:300] + "..."

def generate_lexrank(text):
    text = preprocess_text(text)
    if not text:
        return ""
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        return ' '.join(str(s) for s in lex_summarizer(parser.document, 4)) or text[:300] + "..."
    except Exception:
        return text[:300] + "..."

def generate_bertsum(text):
    if not bert_available:
        return ""
    text = preprocess_text(text)
    if not text:
        return ""

    # Smart truncation
    words = text.split()[:800]
    text = ' '.join(words)

    try:
        summary = bertsum_pipe(
            text,
            max_length=130,
            min_length=30,
            do_sample=False,
            truncation=True,
            num_beams=4
        )
        return summary[0]['summary_text']
    except Exception:
        return "[BERT FAILED]"

# ---- Fixed Processing Function ----
def process_row(idx, row):
    # Access transcript correctly from named tuple
    transcript = row.transcript if hasattr(row, 'transcript') else ""

    # Get all summaries
    textrank = generate_textrank(transcript)
    lexrank = generate_lexrank(transcript)
    bertsum = generate_bertsum(transcript)

    # Convert named tuple to dict if needed
    row_dict = row._asdict() if hasattr(row, '_asdict') else {col: getattr(row, col) for col in df.columns}

    # Store results
    results_with_meta[idx] = {
        **row_dict,
        'textrank_summary': textrank,
        'lexrank_summary': lexrank,
        'bertsum_summary': bertsum
    }

    results_without_meta[idx] = {
        'transcript': transcript,
        'textrank_summary': textrank,
        'lexrank_summary': lexrank,
        'bertsum_summary': bertsum
    }

# ---- Main Processing Loop ----
with tqdm(total=len(df), desc="Processing rows") as pbar:
    for idx, row in enumerate(df.itertuples(index=False)):
        process_row(idx, row)
        pbar.update(1)

# ---- Save Results ----
pd.DataFrame([x for x in results_with_meta if x is not None]).to_csv('summaries_with_metadata.csv', index=False)
pd.DataFrame([x for x in results_without_meta if x is not None]).to_csv('summaries_without_metadata.csv', index=False)

df1 = pd.read_csv('summaries_with_metadata.csv')
df2 = pd.read_csv('summaries_without_metadata.csv')

df1

df2

